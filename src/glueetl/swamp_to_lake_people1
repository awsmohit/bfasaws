import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions  import *
import boto3
from nameparser import HumanName
from pyspark.sql.types import StringType

## @params: [JOB_NAME, region, bfaslakecrawler,gluedatabase]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'region','bfaslakecrawler','lakebucket','gluedatalakedb','gluedataswampdb'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
print ("*****Arguments*****")
print  args

datasource0 = glueContext.create_dynamic_frame.from_catalog(database = args['gluedataswampdb'], table_name = "enterprisedatapersonsedited", transformation_ctx = "datasource0")
print ("******datasource count******")
print (datasource0.count())
if (datasource0.count() == 0):
    print("Dataframe empty - nothing to do!")
    job.commit()
    quit()
## https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-ApplyMapping.html
print "****ApplyMapping*****"
applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("edited date", "string", "edited date", "string"), ("name", "string", "name", "string"), ("street", "string", "street", "string"), ("city", "string", "city", "string"), ("state", "string", "state", "string"), ("zip code", "string", "zip code", "string"), ("phone", "string", "phone", "string"), ("primary email", "string", "primary email", "string")], transformation_ctx = "applymapping1")
print "****ApplyMapping complete*****"
print ("******applymapping1 count******")
print (applymapping1.count())
## https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-ResolveChoice.html
#resolvechoice2 =  ResolveChoice.apply(frame = applymapping1, transformation_ctx = "resolvechoice2",specs = [('zip code','cast:long')])
## https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-DropNullFields.html
print "****DropNullFields*****"
dropnullfields3 = DropNullFields.apply(frame = applymapping1, transformation_ctx = "dropnullfields3")
print "****dropnullfields3 completed*****"
print ("******dropnullfields3 count******")
print (dropnullfields3.count())

#df=dropnullfields3.toDF() \
  #  .withColumn('firstname', HumanName(col('name')).first) \
  #  .withColumn('lastname', HumanName(col('name')).last) \
  
df=dropnullfields3.toDF()
# Register UDF for each name part
humanname_title_udf = udf(lambda z: HumanName(z).title, StringType())
humanname_first_udf = udf(lambda z: HumanName(z).first, StringType())
humanname_middle_udf = udf(lambda z: HumanName(z).middle, StringType())
humanname_last_udf = udf(lambda z: HumanName(z).last, StringType())
humanname_suffix_udf = udf(lambda z: HumanName(z).suffix, StringType())
humanname_nickname_udf = udf(lambda z: HumanName(z).nickname, StringType())

# Now we can use UDFs on Spark dataframe
df2a = df.withColumn('name_title',humanname_title_udf('name')) \
    .withColumn('first_name',humanname_first_udf('name')) \
    .withColumn('middle_name',humanname_middle_udf('name')) \
    .withColumn('last_name',humanname_last_udf('name')) \
    .withColumn('name_suffix',humanname_suffix_udf('name')) \
    .withColumn('name_nickname',humanname_nickname_udf('name')) 
    
addnamessplit = DynamicFrame.fromDF(df2a,glueContext,"addnamessplit")
print "****addnamessplit completed*****"
print ("******addnamessplit count******")
print (addnamessplit.count()) 
print "****Name split complete*****"
datasink = glueContext.write_dynamic_frame.from_options(frame = addnamessplit, connection_type = "s3", connection_options = {"path": "s3://{}/people/".format(args['lakebucket'])}, format = "parquet", transformation_ctx = "datasink")
print "****Name split complete. Printing schema*****"
print ("*****S3 path for parquet output****")
print("s3://{}/people/".format(args['lakebucket']))
# run datalake crawler
print "*****Starting data lake crawler*******"
client=boto3.client('glue', region_name=args['region'])
client.start_crawler(Name=args['bfaslakecrawler'])
job.commit()
print "*****Data lake crawler completed. Job committed******"
