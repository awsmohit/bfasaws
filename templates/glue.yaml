AWSTemplateFormatVersion: '2010-09-09'
Description: This template creates Glue tables and transformations
Parameters:
  BFASDATASTACK:
    Description: Stack that defines IAM roles for Glue and Lambda fucntions
    Type: String
  ETLBUCKET:
    Description: Bucket Name holding ETL code artifacts
    Type: String
  DATASWAMPBUCKET:
    Description: Bucket Name for lake data storage
    Type: String
  
  DATALAKEBUCKET:
    Description: Bucket Name for swamp data storage
    Type: String  
 
  GLUEDATALAKEDB:
    Description: Name of Glue database holding data lake information
    Type: String
    Default: datalakedb
    
  GLUEDATASWAMPDB:
    Description: Name of Glue database holding data swamp information
    Type: String
    Default: dataswampdb
    
  SWAMPFOLDERPATH:
    Type: String
    Default: shelterluv
  
  LAKEFOLDERPATH:
    Type: String
    Default: people
  
  SWAMPCRAWLER:
    Type: String
    Default: bfas-sandbox-glue-crawler-swamp
  
  LAKECRAWLER:
    Type: String
    Default: bfas-sandbox-glue-crawler-lake
  
  GLUEDATALAKEDB:
    Type: String
    Default: bfas-sandbox-glue-database-swamp
  
  GLUEDATASWAMPDB:
    Type: String
    Default: bfas-sandbox-glue-database-lake
  ETLNAME:
    Type: String
    Default: bfas-sandbox-glue-etl-people
  
 
Resources:
  TempBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
    
  BfasSwampDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId   
      DatabaseInput:
        Name: !Ref GLUEDATASWAMPDB	
        Description: Database for all BFAS data swamp tables
  
  BfasLakeDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId   
      DatabaseInput:
        Name: !Ref GLUEDATALAKEDB	
        Description: Database for all BFAS data lake tables
  # STAGE 0 Crawlers 0 scheduled to run every 15 minutes
  # Completion of Stage 0 crawlers fires CloudWatch Events Rule, used to orchestrate

  BfasSwampCrawler:
    Type: AWS::Glue::Crawler 
    Properties:
      Role : { "Fn::ImportValue" : {"Fn::Sub": "${BFASDATASTACK}-oGlueRoleArn"  } }
      
      Name: !Ref SWAMPCRAWLER	
      #bfas-sandbox-glue-crawler-swamp
      Configuration: !Sub |
        {
          "Version": 1.0,
          "CrawlerOutput": {
            "Partitions": { "AddOrUpdateBehavior": "InheritFromTable" },
            "Tables": { "AddOrUpdateBehavior": "MergeNewColumns" }
          }
        }
      Description: "Crawler for ShelterLuv Stage 0 files - output from SherlterLuv"
      DatabaseName: !Ref GLUEDATASWAMPDB
      Targets: 
        S3Targets: 
          - Path: !Sub "s3://${DATASWAMPBUCKET}/${SWAMPFOLDERPATH}"
            
 # STAGE 1 Crawlers - started at end of ETL job scripts

  BfasLakeCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Role : { "Fn::ImportValue" : {"Fn::Sub": "${BFASDATASTACK}-oGlueRoleArn"  } }
      
      Name: !Ref LAKECRAWLER	
      #bfas-sandbox-glue-crawler-lake
      Description: "Crawler for Data Lake"
      DatabaseName: !Ref GLUEDATALAKEDB
      Targets: 
        S3Targets: 
          - Path: !Sub "s3://${DATALAKEBUCKET}/${LAKEFOLDERPATH}"
  # ETL Jobs

  SwamptoLakePeople1:
    Type: AWS::Glue::Job
    Properties:
      Role : { "Fn::ImportValue" : {"Fn::Sub": "${BFASDATASTACK}-oGlueRoleArn"  } }
      
      Name: !Ref ETLNAME	
      #bfas-sandbox-glue-etl-people
      DefaultArguments: {
            "--job-bookmark-option": "job-bookmark-enable",
            "--enable-metrics": "",
            "--TempDir": !Sub "s3://${TempBucket}/01_SwamptoLake",
            "--swampbucket": !Sub "${DATASWAMPBUCKET}",
            "--lakebucket": !Sub "${DATALAKEBUCKET}",
            "--gluedatalakedb": !Sub "${GLUEDATALAKEDB}",
            "--gluedataswampdb": !Sub "${GLUEDATASWAMPDB}",
            "--bfaslakecrawler": !Sub "${BfasLakeCrawler}",
            "--bfasswampcrawler": !Sub "${BfasSwampCrawler}",
            "--region": !Sub "${AWS::Region}",
            "--extra-py-files": !Sub "s3://${ETLBUCKET}/libs/nameparser.zip"
          }
      Command:
        Name: glueetl
        ScriptLocation: !Sub "s3://${ETLBUCKET}/glueetl/swamp_to_lake_people1"
      ExecutionProperty:
        MaxConcurrentRuns: 1
      MaxRetries: 0
      AllocatedCapacity: 30
# CloudWatch Events and Lambda used to orchestrate ETL

  CrawlerSucceededRule:
    Type: AWS::Events::Rule
    Properties: 
      Description: CloudWatch Events Rule for orchestrating crawlers and ETL jobs
      EventPattern: 
        source: 
          - "aws.glue"
        detail-type: 
          - "Glue Crawler State Change"
        detail: 
          state: 
            - "Succeeded"
      State: "ENABLED"
      Targets: 
        - 
          Arn: 
            Fn::GetAtt: 
              - "OrchestrateETLFunction"
              - "Arn"
          Id: "OrchestrateETLFunction"

  OrchestrateETLFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role : { "Fn::ImportValue" : {"Fn::Sub": "${BFASDATASTACK}-oLambdaETLRoleArn"  } }
      Runtime: python2.7
      Timeout: 300
      Code:
        ZipFile: !Sub |
            from __future__ import print_function
            import json
            import boto3
            # define list of jobs to run after each crawler (when new data is found)
            crawler2Jobs={
                "${BfasSwampCrawler}" : ["${SwamptoLakePeople1}"]
                        }
            def handler(event, context):
                print(json.dumps(event))
                crawlerName=event['detail']['crawlerName']
                jobNames=crawler2Jobs.get(crawlerName,[])
                if not jobNames:
                    print("No ETL jobs defined for crawler {}".format(crawlerName))
                else:
                    print("Starting ETL jobs for crawler {}".format(crawlerName))
                for jobName in jobNames:
                    try:
                        print("Starting ETL Job: {}".format(jobName))
                        startJobRun(jobName)
                    except Exception as e:
                        print("Exception thrown: %s" % str(e))
                        pass
                print('Done')
            def startJobRun(jobName):
                client = boto3.client('glue')
                client.start_job_run( JobName=jobName )
                return

  
  PermissionForEventsToInvokeLambda: 
    Type: AWS::Lambda::Permission
    Properties: 
      FunctionName: 
        Ref: "OrchestrateETLFunction"
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: 
        Fn::GetAtt: 
          - "CrawlerSucceededRule"
          - "Arn"

  OrchestrateETLLambdaLogGroup:
    Type: AWS::Logs::LogGroup
    DependsOn: OrchestrateETLFunction
    Properties:
      LogGroupName: !Sub /aws/lambda/${OrchestrateETLFunction}
      RetentionInDays: 7
          
Outputs:
  LAKEDATABASENAME:
    Description: Database Name
    Value: !Ref GLUEDATALAKEDB
    Export:
      Name: !Sub '${AWS::StackName}-BfasLakeDatabase'
   
  SWAMPDATABASENAME:
    Description: Database Name
    Value: !Ref GLUEDATASWAMPDB
    Export:
      Name: !Sub '${AWS::StackName}-BfasSwampDatabase'